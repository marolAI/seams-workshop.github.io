---
slug: io
title: Input & Output (IO)
---

TBD!

Week 1 Morning Discussion Sessions:
{% for ss in site.session %}- [{{ ss.title }}]({{ ss.url | prepend: site.baseurl }})
{% endfor %}

The specific sessions for each topic are similar to those in previous years, though we have consolidated them somewhat; while we finalize this year's materials, please feel free to {% include oldlink.md tx='peruse past sessions' l='sessions' %}.
# Discussion questions ?

<b>Why we should care about the topic and how we should think about it? </b>

-In Scientific work data is a must have.
 
<b>How should we organize and manage our data?</b> 

-Assignment of data management responsibilitiesto named individuals  
-Standard protocols e.g data quality control, data confidentiality standards  
-Security policy for data storage, retention, transmission and destruction  
-Standard data format representations  
-Research data sharing via an institution's repository  
-The need to speedily find your data and related records when you need to  
-Your data and related records are accessible to the relevant people, e.g. collaborators, research funders, other users  
-Your data and related records are disposed of correctly at the end of a project.   
 
<b>How can we automatically gather data?</b>   


-Making use of a system linked to database.  
-Centralised data storage research hubs  
    
<b>How do we share the data ?</b>

Examples:
-Specialist data centre (Most advantagious with more accuracy , with licencing arrangements to aknowledge data rights, data available in most formats)  
-Deposit data in an appropriate Data Repository  
-Post on the project's or institution's web site so data can be accessed globally (often ephemeral/short-lived).  
-Making them available online via a project  
-Making them available informally  between researchers on a peer to pear basis.  
 
<b>How do we capture / document data cleaning / curation / reshaping process? </b>   


<b>Part 1 </b>
 
 1. Implement good practices in a consistent maner to to ensure Data intergrity (data accuracy and consistency)?
 2. Making backups to prevent attacks by viruses
 3. Follow instutional back up policies with regards to back ups
 4. Develop a data storage strategy or policy
 5. Datasecurity arrangements need to be proportionate to the nature of the data and the risks involved. Attention to
    security is also needed when data are to be destroyed
 6. Asign roles and responsibilities to relevant parties in the research team
 7. Know your legal, ethical and other obligations towards participants and funders?  
    
<b>Part 2(Extra Notes)</b>  

This can be achieved through use of Software tools packaged in more cohesive and consistent ways, which lead to:  
1.More efficient code  
2.Easier to remember syntax  
3.Easier to read syntax  

#Examples  
tidyr is a one such R package built for the sole purpose of simplifying the process of creating tidy data. Below are the basic
four fundamental functions of data tidying that tidyr provides:

-gather() makes “wide” data longer  
-spread() makes “long” data wider  
-separate() splits a single column into multiple columns  
-unite() combines multiple columns into a single column  
   

<b>What kind of checkpointing / intermediate outputs / caching should be done?</b> 
1.Calibration of instruments to check the precision, bias or scale of measurement on data entry  
2.Using standardised methods for capturing observations on screens  
3.Setting up validation rules or input masks in data entry software  

<b>What are the performance characteristics (size + access time) of the data format?</b>  
-Different data formats have different capabilities and purposes:  
<b>xml</b>  
<b>Binary</b>  

<b>json</b>  
-JSON is widely used on the web, but because it is based on Javascript data structures,    
it doesn’t seamlessly represent R objects. R attributes like dims and class don’t have equivalents in Javascript.     
Different packages also take different approaches in representing JSON objects in R, and vice versa.  

<b>csv formats </b>  
-CSV is ubiquitous and can be read by most anything but only represents tabular data. Its data types are ambiguous.   
-Msgpack has a data model compatible with JSON but is a binary format. For web applications, replacing JSON with msgpack is an easy way to save bandwidth and CPU usage.    
msgpack is a convenient wire format for embedded devices.  

<b>R functions </b>  
-serialize() is your best best for getting R objects out of and back into R the way you had them, but doesn’t do much for you when communicating with other systems.  
-dump() is your next best bet, while being a somewhat human-readable text format.  
-Neither of above are good for exchanging data with other entities you don’t trust.  

<b>For outputs: how do we make them repeatable (including stochastic ones).?</b>  
 -Make several random calls using input data and save the objects for a rerun when need be  
 
<b>Which formats for which tasks?</b>  
-Example in simulating weather formats ie raw text, structured text (csv), binary, json, sql or other unique  
 output formats kml
1.The safest option to guarantee long-term data access is to convert data to standard  
  formats that most software are capable of interpreting, and that are suitable for data interchange and transformation.  
-All digital data may be endangered by the obsolescence of the hardware and software environment on which access to data depends.  

2.Size and characteristics of the data  
-Give examples of how text files impact the perfomance of your system because they have to be parsed every time. Text files also      
 have an implicit format (each column is a certain value) and if you are not careful documenting this, it can cause problems down the line.  
-Columar formats offer advantage in terms of query speed e.g Select query to search  
-Knowing and the preparedness for Schema evolution as a result of the data changing(addition of new columns). Certain file formats handle schema evolution very well.  
    
3.Project infrastructure  
-Looking at the technologies you’ve chosen to use, and their characteristics; this includes tools used for ETL(Extract, Transform and Load) processes as wells as tools used to   
query and analyze the data.    
This information will help you figure out which format you’re able to use.  
    
4.Use case scenarios  
-decision on which format to use should be based on your specific use cases and systems.the idea is to improve on the speed of reading and writing data.  
-To optimise query perfomance it is important to have columnar data so that searches can be done based on the column criteria. Searching all columns results in more time for    
 output generation.  
     
<b>What is a database ? </b>  
-A database consists of a number of interrelated tables.   
-Each table has a number of records which are used to represent real world objects.  
-Each record has a number of fields which are data items used to specify a characteristic of the record. Examples of fields  
 (Name, gender and age) with structures to specify the types of data per each field e.g (integer, Characters,)    
-A database management system is a Relational Database Management System (RDBMS) if different tables are related to each other by    
 common fields, so that information from several tables can be combined.  

<b>When do i use a database?</b>  
-When we want to store and retrieve information. Databases guarantee persistent, reliable access to the data and provide the    
 ability to co-relate data that gets produced in different areas to understand relationships, generate reports to predict trends  
 for the future.     
-Databases are critical to delivering the immediate, personalized, data-driven applications and real-time analytics.   

<b>An example of a Trade off between text files and csv files.</b> 

<b>A. Text Files </b>  
- Lack of standards:    
- No standard way to specify data format. No standard way to express “special characters”.    
- Inefficiency:    
- Can lead to massive redundancy (repetition of values). Speed of access and space efficiency for large data sets.  Difficult to store “non-rectangular” data sets.    
- Lack of data integrity:  
- lack of data integrity measures  
![Figure 1.1.]() 

```   
Consider a data set collected on two families from the above figure 1.1 .What would this look like as a flat file?    
One possible comma-delimited format is shown below:    
```
```
John,33,male,Julia,32,female,Jack,6,male  
John,33,male,Julia,32,female,Jill,4,female  
John,33,male,Julia,32,female,John jnr,2,male  
David,45,male,Debbie,42,female,Donald,16,male  
David,45,male,Debbie,42,female,Dianne,12,female  

```
-Firstly, it is not efficient; the parent information is repeated over and over again.    

<b>B. csv files </b>  
-Disadvantages of plain CSV files  

Just the storage of this number of files presents a challenge:  
1.Storing these files in one directory would cause confusion making it difficult to find files and would make it difficult to remove or modify files.  
2.The next problem is how to name these files. Choosing file names is a form of documentation; the name of the file   should clearly describe the contents of the file, or at least distinguish the contents of the file from the contents of other files in the same directory.  
4.Another thing to consider is how the files will be ordered in directory listings; will it be easy to browse a list of the files in a directory and find the file we want?   

<b>Database solution to the above scenario </b>  

<b>Balance between plain text and human readable string data </b>  
*All data can be parsed by a suitably equipped and programmed computer or machine; reasons for choosing binary formats over text formats    
 usually center on issues of storage space, as a binary representation usually takes up fewer bytes of storage, and efficiency of access (input and output) without parsing or conversion.  

<!--No content as yet-->

<!-- End of Io Discussion Session Material @Perceval-->






<!--Material from previous presentations-->
<!--

## Communicating with the outside world
 - examples?

## Important context
 - interactive, human user?
 - quick and dirty?
 - need for speed?
 - rigid specification?

## The options
 {% comment %}
 - STDIO
 - ad hoc text
 - csv, tab, etc.
 - HTML, XML, JSON, etc.
 - binary
 - database
 - specialized file formats (e.g., tiff, hdf5, docx)
{% endcomment %}

 Spend 15-20 minutes researching one of the following topics (to be assigned).
 Write down your answers and be prepared to tell the class what you found.
 1. What are standard out and standard error?  What's the difference, and how 
 do you write to them on the command line and in [your language here].
 2. What is a markup language, and what are some examples?  What advantages
 and disadvantages do markup languages have over simple text?
 3. In your own words, what is database normalization?  What's the point?
 4. Choose a specialized file format, explain when it should be used,
 and what advantage it has over plain text.  

## Parsers
 - existing, established
 - making your own (use the standard, write tests, be fastidious)

{% comment %}
Making choices about input formats: raw text, structured text (e.g., csv),
binary, databases.

What should be input?  Obviously empirical data - slightly less obvious
simulation parameters, even less obvious analysis configuration parameters.
However, often very valuable to be able to have configuration of setup / results
as an input.  Importance of random seed as input.

Making choices about output.  Checkpointing.  Value of checkpointing to debugging,
but also scaling up to supercomputer approaches, use in alternative analysis /
visualization streams or handing off to other researchers.  What to save as
interim results.

What to save as "final" results, and how to save it.  Value of having simulation
outputs AND separate visualization, not just final plots.
{% endcomment %}
-->
<!--Material from past presentations -->